---
layout: post
title: 42. Fleruppgiftsinlärning för historisk textnormalisering
---

Läser ännu en rekommenderad artikel på temat textnormalisering: *Few-shot and zero-shot learning for historical text normalization* (Bollmann 2019). Den handlar om hur man kan använda så kallad **multitask learning** (som jag väljer att kalla fleruppgiftsinlärning) för att förbättra resultaten när man har att göra med små träningsset. Bollmann jämför 63 sådana multi-task-arkitekturer för sekvens-till-sekvens-baserad historisk textnormalisering på datasamlingar från åtta olika språk: engelska, tyska, ungerska, isländska, portugisiska, slovenska, spanska och svenska.   

Fleruppgiftsinlärning (MLT) går ut på att försöka förbättra generaliseringen genom att träna en modell på en uppsättning relaterade uppgifter, som exempelvis lemmatisering och grafem-till-fonem-mappning. Bollmann refererar Caruana (1993) som verkar vara den som introducerat begreppet. Caruana (1993:1) beskriver motivationen bakom MLI så här:

>"The standard methodology in machine learning is to break large problems into small, reasonably independent subproblems, learn the subproblems separately, and then recombine the learned pieces (Waibel 1989). This paper suggests that part of this methodology may be wrong. The reductionist method has caused us to attempt learning on simple, isolated tasks before earnestly attempting learning on larger, richer tasks. By adhering to this method we may be ignoring a critical source  of inductive bias  for real-world problems: the inductive bias inherent in the similarity of related tasks drawn from  the same  domain." 

I Bollmanns studie används [kodare-avkodare-modeller med uppmärksamhet](https://datatjej.github.io/Kodare-avkodare-modeller-och-uppm%C3%A4rksamhet/) som tar ord som indatasekvenser och tecken som indatasymboler. En källinbäddningslager (eng. *source embedding layer*) omvandlar bokstäverna till täta vektorer (där många av värdena är noll) som därefter passerar genom en kodare (ett dubbelriktad LSTM-lager) och ett uppmärksamhetslager (som beräknar uppmärksamheten utifrån den inkodade indatasekvensen och nuvarande avkodartillståndet mha. av en flerlagrig perceptron). Ett målinbäddningslager (eng. *target embedding layer*) omvandlar utdatabokstäverna till täta vektorer som avkodaren (ett annat LSTM) avkodar ett tecken i taget mha. uppmärksamhetsvektorn. Ett sista framåtriktat prediktionslager gör en softmax-bearbetning av sannolikhetsfördelningen över alla möjliga utdatatecken. 

MLT med **hård parameterisering** sker genom att de olika uppgifterna som studien testar på -- autoencoding (avkodning?), grafem-till-fonem och lemmaterisering -- har vissa av komponenterna i den ovan beskrivna modellarkitekturen gemensamma. Det går att dela på alla komponenter -- i så fall tränar man i praktiken en modell på samtliga uppgifter -- alternativt dela på allt förutom prediktionslagret eller mindre än så. Bollmann testar alla möjliga konfigurationer, 63 stycken, och kommer fram till att MLI leder till förbättrade resultat både vid så kallad *few-shot-inlärning* (när träningsdatat består av så lite som 1000 tokens) och även vid *zero-shot-inlärning* (där träningsdata saknas för några av kategorierna): *"In total, 49 configurations outperform the single-task model, showing the general effectiveness of the MTL approach.*" (Bollmann 2019: 106). Han konstaterar även att det blir bättre resultat ju fler komponenter som delas: *"[...]  nine out of the top ten configurations share at least four components"* (ibid). 

Apropå valet av "auxiliary tasks", dvs relaterade sidouppgifter, skriver han att inte alla uppgifter visade sig lika användbara. Autoencoding (följt av lemmatisering) var den sidouppgift som mest främjade den historiska textnormaliseringen, vilket verkar ha att göra med att den hindrar modellen från att övergeneralisera från träningsdatat. Detta eftersom indata och utdata för autoencoding-uppgiften är ganska lika, något de också är vid textnormalisering (om jag förstått det hela rätt). Den positiva effekten av autoencoding och MLT generellt avtar dock när mängden träningsdata ökar.     

**Referenser**

* Marcel Bollmann, Natalia Korchagina och Anders Søgaard. 2019. [Few-Shot and Zero-Shot Learning for Historical Text Normalization](https://www.aclweb.org/anthology/D19-6112.pdf). I *Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP* (DeepLo 2019), sidorna 104–114, Hong Kong. Association for Computational Linguistics.

* Rich Caruana. 1993. [Multitask learning: A knowledge-based source of inductive bias](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.57.3196&rep=rep1&type=pdf). I *Proceedings  of  the  10th  International  Conference  on Machine Learning* (ICML), sidorna 41–48.

* Alexander Waibel,  Hidefumi Sawai och Kiyohiro Shikano. 1989. [Modularity and Scaling in Large Phonemic Neural Networks](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=45535). I *IEEE Transactions on Acoustics, Speech and Signal Processing*. 37(12): 1888-98.