---
layout: post
title: 32. GPU:er, AKA "pinsamt parallella"
---

**GPU** står för *graphics processing units* och är hårdvaran som underlättar maskin- och djupinlärning. I skolan använder vi MLT-programmets egna GPU-processorer ("mltgpu") för maskininlärning, inklusive NLU-träning (*natural language understanding*) med Rasa. GPU:er kan ses i kontrast till CPU:er (*central processing units*), centralprocessorer, som är chip som finns i alla datorer och fungerar som hjärnan i datorn. CPU:er tar in instruktioner från datorprogram och utför **generella beräkningar** för att hantera programflöden och minnesdelning. De skickar ut signaler till externa enheter och tar även emot signaler från exempelvis tangentbordet. Tyvärr blir CPU:er ganska **långsamma** om man vill utföra de beräkningar som görs inom maskininlärning eftersom allting sker **sekventiellt**. 

För den typen av **specialiserade beräkningar** lämpar sig istället GPU:er bättre där alla beräkningar sker **parallellt**. Häromdagen stöte jag på begreppet **"embarrassingly parallel"** apropå GPU:er, vilket verkar vara ett [etablerat begrepp](https://en.wikipedia.org/wiki/Embarrassingly_parallel). Det har sitt ursprung i att dessa beräkningar visserligen är **intensiva** men inte nödvändigtvis komplicerade eftersom allting kan brytas ner i mindre, oberoende räkneuppgifter.  

GPU:er utvecklades från början för dataspelsgrafik där man snabbt vill manipulera skärmens pixlar som var och en representerar en upsättning siffror för färgkoder. Den här manipuleringen av stora sifferområden är väldigt lik **matris- och tensoroperationerna** som utförs inom maskininlärning, med den enda skillnaden att vi inte manipulerar konkreta bilder på skärmen, utan mer abstrakta entititer. Något som bidrar till att göra GPU:erna snabbare än CPU:erna är **antalet processorkärnor** (eng. *cores*). Medan CPU:er brukar ha 4, 8 eller 16 kärnor så kan GPU:er ha hundratals [[1]](https://www.youtube.com/watch?v=6stDhEA0wFQ), [[2](https://www.youtube.com/watch?v=LfdK-v0SbGI)]]. 

Skolans mltgpu består av fyra GPU:er som köptes in för ungefär 3 år sedan. Eftersom varje GPU har ett begränsat minne måste vi dela på resurserna och vara medvetna om vilken av dessa fyra vi använder. För att kolla GPU:ernas status kan man köra kommandot `nvidia-smi`. **Nvidia** är tillsammans med **AMD** en av de två stora tillverkarna av GPU:er. 

<p align="center">
<img src="/images/nvidia-smi.PNG" alt="Resultatet av kommandot nvidia-smli" width="100%" height="auto" border="10" /><br>
Tabellen visar de fyra GPU:erna och hur mycket av deras minne (11mb var) som används samt wattförbrukning. Den undre rutan visar vilka processer som pågår just nu.
</p>   

In pytorch programs you often have to specify which one you are using as to not hog the whole thing. i encourage you to roll a dice to decide which one is going to be used at random. 


--------------------------       


GPUs often faster, but not always, only if the operations can be done in parallel. moving data from the cpu to the gpu is costly, so when we do this, the overall performance might be slower if the computation is a simple one. remember, the GPU works well for tasks that can be broken down into several smaller tasks. 

CPUs generally have 4, 8 och 16 cores, while GPUs have potentially thousands of cores. "GPUs are embarassingly parallell"

NVIDIA GPU is the hardware. CUDA is the software layer that provides the API for developers.  once you have the gpu, cuda can be downloaded and installed from the nvidia website for free. 

cuda toolkit - specialized libraires, like cuDNN (cuda deep learning neural network), with pytorch, cuda comes baked in from the start, and we don't need to know how to use the cuda api to work on a pytorch core development team or write pytorch extensions. 

pythorch is written in python, however, at bottleneck points, pytorch drops into C++ and cuda to speed up processing and get that performance boost. 

if we create a tensor, then it automatically ends up on the CPU. by calling the cuda() function on it, we move it to the GPU.

GPGPU computing - general purpose gpu (2014) - nvidia has been a pioneer in this space
---------------

GPU Computating
1) simulation (drug design, seismic imaging, automotive design)
2) vizualisation 

---------------------
Some parts of an app code might be offloaded to the GPU because it requires intense computations. You can think of the GPU as the extra brainpower that the CPU cannot manage on its own. SO there are two main providers of GPUs in industry: nvidia and AMD. Both providers manufacture GPUs that are optimized for certain use cases. So let's jump into that. 

USE CASES:
- VDI (virtual desktop infrastructure)
...so GPUs are created to support high intensity graphics applications. 
3D CAD program on a server a country away - supported by GPU
movie animation
or rendering

But gaming is no longer the focus in industry anymore. It's a big piece of it, but now financial services, life sciences and even health care are starting to get into it with Artificial Intelligence (machine learning and deep learning - there are GPUs that are optimized and created specifically for those applications, for inferencing for machine learning purposes and others that are created to help data scientis create and train neural networks, "to think like a human brain", which CPUs can't handle).  

Lastly let's talk about HPC. Buzzword: high-performance computing. While GPU is not absolutely necessary for HPC, it's an important part of it. So HPC is the company's ability to spread about their compute-intensive workloads amongst multiple compute nodes -- or in the case of cloudes: servers. Oftentimes though these applications are very compute-intensive - it could include rendering, it could include AI - and that's where the GPU comes in. You can add a GPU to these servers that are spread out among a HPC application and utilize those in that manner. 

So why should we use GPU on cloud?  
- high performance (GPUs are great, but not on their own). back in the day, and still today, there are companies that use a lot of on-perm infrastructre, and they utilize that infrastructure for any of their compute-intensive applications. however, especially in the case of GPUs, the technology is everychanging. In fact, there's typically a new GPU coming out every single year. So it's very expensive and nearly impractical for companies to keep up with the latest technology at this point. So cloud providers have the ability to continously update their technology and provide GPUs to these companies to utilize them when they need them.    
- bare metal vs virtual servers 
there are advantanges of using GPU on both types of infrastructure. on a bare metal infrastructure, the company usually have access to the entire server itself and they can customize their configurations. so this is great for companies that are gonne be really utilizing that server and that GPU-intense application on a pretty constitant basis. but for companies that need a gpu maybe just on a worst/burst workload scenario, the virtual server option might be even better. and the nice thing about virtual is that they are offer different pricing models as well, including hourly. And the cool thing about the cloud is that you only pay for what you use. 
