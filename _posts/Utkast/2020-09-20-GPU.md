---
layout: post
title: 32. GPU:er, AKA "pinsamt parallella"
---

**GPU** står för *graphics processing units* och är hårdvaran som underlättar maskin- och djupinlärning. I skolan använder vi MLT-programmets egna GPU-processorer (mltgpu) för maskininlärning, inklusive NLU-träning (*natural language understanding*) med Rasa. GPU:er kan ses i kontrast till CPU:er (*central processing units*), centralprocessorer, som är chip som finns i alla datorer och fungerar som hjärnan i datorn. CPU:er tar in instruktioner från datorprogram och utför **generella beräkningar** för att hantera programflöden och minnesdelning. De skickar signaler till externa enheter och tar även emot signaler från exempelvis tangentbordet. Tyvärr blir dessa ganska **långsamma** om man vill utföra de beräkningar vi gör inom maskininlärning.     

För den typen av **specialiserade beräkningar** lämpar sig GPU:er bättre. De utvecklades i början för dataspelsgrafik där man snabbt vill manipulera skärmens pixlar som var och en representerar en upsättning siffror för färgkoder. Den här manipuleringen av stora sifferområden är väldigt lik **matris- och tensoroperationerna** som utförs inom maskininlärning, med den enda skillnaden att vi inte manipulerar konkreta bilder på skärmen, utan mer abstrakta entititer.     

Skolans mltgpu består av fyra GPU:er som köptes in för ungefär 3 år sedan. Detta är relativt gammalt med tanke på att de stora tillverkarna släpper en ny GPU i princip varje år, men de är kraftfulla och fullt tillräckliga för våra behov. Eftersom varje GPU har ett begränsat minne måste vi dock dela på resurserna och vara medvetna om vilken av dessa fyra vi använder. För att kolla GPU:ernas status kan man köra kommandot `nvidia-smi`. 

<p align="center">
<img src="/images/nvidia-smi.PNG" alt="Resultatet av kommandot nvidia-smli" width="100%" height="auto" border="10" /><br>
Tabellen visar de fyra GPU:erna och hur mycket av deras minne (11mb var) som används samt wattförbrukning. Den undre rutan visar vilka processer som pågår just nu.
</p>   

In pytorch programs you often have to specify which one you are using as to not hog the whole thing. i encourage you to roll a dice to decide which one is going to be used at random. 


--------------------------       


GPUs often faster, but not always, only if the operations can be done in parallel. moving data from the cpu to the gpu is costly, so when we do this, the overall performance might be slower if the computation is a simple one. remember, the GPU works well for tasks that can be broken down into several smaller tasks. 

CPUs generally have 4, 8 och 16 cores, while GPUs have potentially thousands of cores. "GPUs are embarassingly parallell"

the CONVOLUTION OPERATION is often used in deep learning. we have an input channel in blue on the bottom, a convolutional filter shaded on top of the input channel (sliding across) and a green output channel. for each positiion of the convolutional filter, there is a corresponding green region (the output of the convolutional operation at each point) 

NVIDIA GPU is the hardware. CUDA is the software layer that provides the API for developers.  once you have the gpu, cuda can be downloaded and installed from the nvidia website for free. 

cuda toolkit - specialized libraires, like cuDNN (cuda deep learning neural network), with pytorch, cuda comes baked in from the start, and we don't need to know how to use the cuda api to work on a pytorch core development team or write pytorch extensions. 

pythorch is written in python, however, at bottleneck points, pytorch drops into C++ and cuda to speed up processing and get that performance boost. 

if we create a tensor, then it automatically ends up on the CPU. by calling the cuda() function on it, we move it to the GPU.

GPGPU computing - general purpose gpu (2014) - nvidia has been a pioneer in this space
---------------

GPU Computating
1) simulation (drug design, seismic imaging, automotive design)
2) vizualisation 

how do you decide how many cuda cores and tensor cores to put on a chip?

destiny meets serendipity

NVIDIA introduced GPU in 1999, the goal of the GPU was to "accelerate computationally-intensive applications" and "approach the image quality of movie studo offline rendering farms, but in real-time"

--> accelerate PC gaming and PC graphics

instead of hours per frame > 60 frames per second

Millions of pixels per frame can all be operated on in prallel

3D graphics is often termed embarrassingly parallel

Use large arrays of floating point units to exploit wide and deep parallelism. 

Gefore 6 and 7 series (2004-2006)
278M transistors

in GPU, we render an image as a collection of triangles

NUMERIC REPRESENTATIONS
- a variety of numeric formats inside a GPU

G80: REDEFINED THE GPU
- released in 2006 
- first GPU with a unified shader processor architecture
- introduced SM (Streaming Multiprocessor): array of simple streaming processor cores: CUDA cores (previously "SPs")
- all shader stages use the same instruction set
- all shader stages execute the same units
- permits better sharing of SM hardware resources
- recognized that building dedicated units often results in under-utilization due to the application workload ("share the common array")
- 681 transistors
- beginning of CUDA everywhere
- beginning of GPU computing 

FERMI GF 100:
- 2011
- 3B transistors
- 247 watt

KEPLER GK 110
- 7.1B transistors
- 235 watt

PASCAL GP100
-15B transistors
- 300 watt


************************
CPUs are made up of just a few cores. You can think of these cores as the power or the ability of a CPU to do certain calucations or computations. GPUs are made up of hundreds of cores. When the CPU does the computation, it does so in a **serial form**: one computation at a time. With the GPU, it does it in **parallell** -- all at once. Very intense computations.  

Some parts of an app code might be offloaded to the GPU because it requires intense computations. You can think of the GPU as the extra brainpower that the CPU cannot manage on its own. SO there are two main providers of GPUs in industry: nvidia and AND(?). Both providers manufacture GPUs that are optimized for certain use cases. So let's jump into that. 

USE CASES:
- VDI (virtual desktop infrastructure)
...so GPUs are created to support high intensity graphics applications. 
3D CAD program on a server a country away - supported by GPU
movie animation
or rendering

But gaming is no longer the focus in industry anymore. It's a big piece of it, but now financial services, life sciences and even health care are starting to get into it with Artificial Intelligence (machine learning and deep learning - there are GPUs that are optimized and created specifically for those applications, for inferencing for machine learning purposes and others that are created to help data scientis create and train neural networks, "to think like a human brain", which CPUs can't handle).  

Lastly let's talk about HPC. Buzzword: high-performance computing. While GPU is not absolutely necessary for HPC, it's an important part of it. So HPC is the company's ability to spread about their compute-intensive workloads amongst multiple compute nodes -- or in the case of cloudes: servers. Oftentimes though these applications are very compute-intensive - it could include rendering, it could include AI - and that's where the GPU comes in. You can add a GPU to these servers that are spread out among a HPC application and utilize those in that manner. 

So why should we use GPU on cloud?  
- high performance (GPUs are great, but not on their own). back in the day, and still today, there are companies that use a lot of on-perm infrastructre, and they utilize that infrastructure for any of their compute-intensive applications. however, especially in the case of GPUs, the technology is everychanging. In fact, there's typically a new GPU coming out every single year. So it's very expensive and nearly impractical for companies to keep up with the latest technology at this point. So cloud providers have the ability to continously update their technology and provide GPUs to these companies to utilize them when they need them.    
- bare metal vs virtual servers 
there are advantanges of using GPU on both types of infrastructure. on a bare metal infrastructure, the company usually have access to the entire server itself and they can customize their configurations. so this is great for companies that are gonne be really utilizing that server and that GPU-intense application on a pretty constitant basis. but for companies that need a gpu maybe just on a worst/burst workload scenario, the virtual server option might be even better. and the nice thing about virtual is that they are offer different pricing models as well, including hourly. And the cool thing about the cloud is that you only pay for what you use. 
