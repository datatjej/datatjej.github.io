---
layout: post
title: 22. Vektorrumsmodeller
---
>"We draw attention to the fact that VSMs are arguably the most successful approach to semantics, so far. (Turney & Pantel 2010)
"

Inför veckans semantikseminarium fick vi som hemläxa att läsa tre artiklar om distributionell semantik och **vektorrumsmodeller**. En av dem var översiktsartikeln ["From Frequency to Meanings: Vector Space Models of Semantics"](https://arxiv.org/abs/1003.1141) av Turney & Pantel (2010) som går igenom de tre övergripande vektorrumsmodellerna (VRM): 1) **term-dokument**, 2) **ord-kontext** och 3) **par-mönster-matriser**. 

VRM utvecklades i början av 70-talet av datorforskaren [Gerard Salton](https://en.wikipedia.org/wiki/Gerard_Salton) och kollegor för informationshämtningssystemet SMART vid Cornell University. Det gjorde det möjligt att representera varje dokument i en dokumentsamling som en punkt i ett vektorrum, med avståndet mellan punkterna som ett uttryck för graden av semantisk likhet mellan dem. Även användarens söksträng blir då till en punkt i samma vektorrum, ett *pseudodokument*, och dokumenten presenteras sedan för användaren i den ordning som de ligger avståndsmässigt från söksträngens vektor (Turney & Pantel 2010). Det här fungerade så bra i informationshämtningssammanhang att man började applicera VRM även på andra språkteknologiska problem.

Vektorer hade använts inom AI och kogntitivforskningen redan innan Saltons VRM, men författarna till artikeln väljer att skilja VRM från annan användning av vektorer och matriser genom att sätta som krav att vektorerna har sitt ursprung i **händelsefrekvenser** (eng. *event frequencies*) av olika slag. Det nya med Saltons VRM var just att man nu använde **frekvenser i en textkorpus** för utröna semantisk information. Tittar man exempelvis på klassiska **maskininlärningsuppgifter** som klassificering och klustring av olika objekt så brukar dessa representeras som **feature vectors** (kanske *egenskapsvektorer* på svenska?) som *kan* ha sitt ursprung i händelsefrekvenser, men inte måste ha det. Ett annat exempel är **rekommendationssystem** där man ofta har en **person-objekt-matris** bestående av rader med kunder och kolumner med produkter som de betygsatt på olika sätt. Person-objekt-matrisen delar vissa matematiska beräkningslikheter med term-dokument-matriser, men förlitar sig alltså *inte* på händelsefrekvenser. 

<p align="center">
<img src="/images/kund_produkt_matris.jpg" alt="Kund-produkt-matris" border="10" /> <br>
Kund-produkt-matris. Ett exempel på vektoranvändning men *inte* en vektorrumsmodell eftersom värdena är betyg och inte händelsefrekvenser.</p>

VRM har vanligtvis text som input - ibland i råformat, men oftast efter att den först genomgått någon slags språklig behandling: trunkering (genom en [stemmer](https://datatjej.github.io/Porter-stemmern/)), POS-taggning, word sense-taggning eller parsning (Turney & Pantel 2010). I enklare VRM av typen term-dokument-matris består värdet hos elementet i en dokumentvektor av antalet gånger som motsvarande ord förekommer i dokumentet. Men ofta utför man ytterligare matematiska operationer på den råa ordfrekvensen, såsom viktning, matrisutjämning (eng. *smoothing*) och vektorjämförelse (ibid). 

**Term-dokument-matriser** används för att jämföra dokument. Där består raderna av termer och kolumnerna av dokument. Dokumentvektorerna representerar dokumenten som en så kallad **bag of words** (ordsäck? ¯\_(ツ)_/¯), där en **bag** (eller **multimängd** på svenska, eng. *multiset*) är ett matematiskt begrepp som syftar på en **mängd** (eng. *set*) som tillåter dubletter, t.ex. {a,b,b,b,c}. Precis som i en vanlig mängd så är ordningen på elementen irrelevant. Multimängden {a,b,b,b,c} blir i vektorformat **v** = <1,3,1>. En **mängd av multimängder** kan representeras som en matris där kolumnerna är multimängder (ordsäckar) och raderna unika element. **Bag of words-hypotesen** säger att vi kan uppskatta dokumentrelevansen för en söksträng genom att representera både dokumenten i en dokumentsamling och själva söksträngen som *bags of words*. Det ger upphov till en **gles matris** eftersom de flesta dokument bara innehåller en bråkdel av orden i det gemensamma ordförrådet. Den här modellen **fångar inte upp ordföljden** eller annan syntaktisk struktur i dokumenten. 

**Ord-kontext-matrisen** fokuserar istället på **likhet mellan ord** genom att byta ut dokumenten i term-dokument-matrisen till en mindre kontext som exempelvis ett **ordfönster** (de ord som står några snäpp till vänster och höger om **målordet**) eller **grammatiska dependenser**. Den här modellen, först framförd av Deerwester et. al. år 1990, kopplar an till den **distributionella hypotesen** som säger att ord som förekommer i liknande sammanhang också tenderar att ha liknande betydelser.                                

**Par-mönster-matrisen**, introducerad av Lin & Pantel (2001), beskriver slutligen **likhet mellan ordpar och relationer**. Där består raderna av termer i par, t.ex. *brevbärare:post* och *dataforskare:data*, och kolumnerna av mönster där termparen förekommer, t.ex. *X arbetar med Y*. Man föreslog därmed en slags **utökad distributionell hypotes** som säger att mönster som förekommer med liknande ordpar tenderar att ha liknande betydelse. Om mönstret *X löser Y* tenderar att förekomma med samma X:Y-ordpar som mönstret *Y löses av X* kan man anta att de båda mönsterna uttrycker ungefär samma sak. Turney et. al. (2003) använde par-mönster-matrisen för att undersöka likheten i semantiska relationer mellan ordpar som förekommer i liknande mönster. Ordpar som *keramiker:lera* och *glasblåsare:glas* delar exempelvis den semantiska relationen *hantverkare:material*, vilket gett upphov till **latent relation-hypotesen**.   

Oavsett vilken av ovanstående matriser man använder sig av så följer VRM-byggandet ofta följande fyra steg:<br>
1) beräkna frekvenserna (och lagra i en hashtabell, databas eller sökmotorsindex)<br> 
2) bearbeta den råa ordfrekvensen (t.ex. genom att vikta dem så att förvånande händelser får större vikt än de förväntade händelserna)<br>
3) jämna ut vektorrummet (dimensionsreducering)<br>
4) beräkna likheterna<br>
Apropå punkt 2 så kan man tänka sig att för orden *hund* och *katt* så leder kontextorden *klappa* och *mata* till större semantisk likhet än orden *ha* eller *se*, och borde därför få större vikt. **Tf-idf**, som jag har skrivit lite om tidigare [här](https://datatjej. github.io/Tf-idf/), är ett sätt att säkerställa att ovanligare ord som "klappa" och "mata" får större vikt än ord som förekommer flitigt i hela dokumentsamlingen. En annan slags viktningsmekanism i informationshämtningssammanhang är **längdnormalisering** som säkerställer sökmotorer inte favoriserar större dokument per automatik.
